# -*- coding: utf-8 -*-
"""Predicting_Microbe_Killing_PEPTIDES.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z0AQ3nVcC6yUOU1MjvYqnfAW_ImFZfGc

# **Predicting Whether Given peptide is microbicidal Using Machine Learning Classifier**

# **Install conda**
"""

################################################################################
# INSTALL CONDA ON GOOGLE COLAB
################################################################################
! wget https://repo.anaconda.com/miniconda/Miniconda3-py37_4.8.2-Linux-x86_64.sh
! chmod +x Miniconda3-py37_4.8.2-Linux-x86_64.sh
! bash ./Miniconda3-py37_4.8.2-Linux-x86_64.sh -b -f -p /usr/local
import sys
sys.path.append('/usr/local/lib/python3.7/site-packages/')

"""# **Install Pfeature**"""

! wget https://github.com/raghavagps/Pfeature/raw/master/PyLib/Pfeature.zip

! unzip Pfeature.zip

# Commented out IPython magic to ensure Python compatibility.
# % cd Pfeature

! python setup.py install

"""# **Install CD-HIT**"""

! conda install -c bioconda cd-hit -y

"""# **Load peptide dataset (text files)**"""

! wget https://raw.githubusercontent.com/dataprofessor/AMP/main/train_po.fasta

! wget https://raw.githubusercontent.com/dataprofessor/AMP/main/train_ne.fasta

! cat train_ne.fasta

"""# **using CD-HIT to remove redundant sequences**"""

! cd-hit -i train_po.fasta -o train_po_cdhit.txt -c 0.99

! cd-hit -i train_ne.fasta -o train_ne_cdhit.txt -c 0.99

! ls -l

! grep ">" train_po_cdhit.txt | wc -l

! grep ">" train_po.fasta | wc -l

! grep ">" train_ne.fasta | wc -l

! grep ">" train_ne_cdhit.txt | wc -l

"""# **Calculate features using the Pfeature library**

### **Define functions for calculating the different features**
"""

import pandas as pd

# Amino acid composition (AAC)

from Pfeature.pfeature import aac_wp

def aac(input):
  a = input.rstrip('txt')
  output = a + 'aac.csv'
  df_out = aac_wp(input, output)
  df_in = pd.read_csv(output)
  return df_in

aac('train_po_cdhit.txt')

# Dipeptide composition (DPC)

from Pfeature.pfeature import dpc_wp

def dpc(input):
  a = input.rstrip('txt')
  output = a + 'dpc.csv'
  df_out = dpc_wp(input, output, 1)
  df_in = pd.read_csv(output)
  return df_in

feature = dpc('train_po_cdhit.txt')
feature

"""### **Calculate feature for both positive and negative classes + combines the two classes + merge with class labels**"""

pos = 'train_po_cdhit.txt'
neg = 'train_ne_cdhit.txt'

def feature_calc(po, ne, feature_name):
  # Calculate feature
  po_feature = feature_name(po)
  ne_feature = feature_name(ne)
  # Create class labels
  po_class = pd.Series(['positive' for i in range(len(po_feature))])
  ne_class = pd.Series(['negative' for i in range(len(ne_feature))])
  # Combine po and ne
  po_ne_class = pd.concat([po_class, ne_class], axis=0)
  po_ne_class.name = 'class'
  po_ne_feature = pd.concat([po_feature, ne_feature], axis=0)
  # Combine feature and class
  df = pd.concat([po_ne_feature, po_ne_class], axis=1)
  return df

feature = feature_calc(pos, neg, aac) # AAC
#feature = feature_calc(pos, neg, dpc) # DPC
feature

"""# **Data pre-processing**"""

feature

# Assigns the features to X and class label to Y
X = feature.drop('class', axis=1)
y = feature['class'].copy()

# Encoding the Y class label
y = y.map({"positive": 1, "negative": 0})

X.shape

# Commented out IPython magic to ensure Python compatibility.
#heatmap
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
  
# plotting correlation heatmap
sns.set(rc = {'figure.figsize':(18,18)})
dataplot = sns.heatmap(X.corr(), cmap="YlGnBu")

# displaying heatmap
plt.show()

# Feature selection (Variance threshold)
from sklearn.feature_selection import VarianceThreshold

fs = VarianceThreshold(threshold=0.1)
fs.fit_transform(X)
#X2.shape
X2 = X.loc[:, fs.get_support()]
X2

# Data split
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X2, y, test_size=0.2, random_state =42, stratify=y)

"""---

# **Quickly compare >30 ML algorithms**
"""

! pip install lazypredict

# Import libraries
import lazypredict
from lazypredict.Supervised import LazyClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import matthews_corrcoef

# Load dataset
X = feature.drop('class', axis=1)
y = feature['class'].copy()

# Data split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state =42, stratify=y)

# Defines and builds the lazyclassifier
clf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=matthews_corrcoef)
models_train,predictions_train = clf.fit(X_train, X_train, y_train, y_train)
#models_test,predictions_test = clf.fit(X_train, X_test, y_train, y_test)

# Prints the model performance (Training set)
models_train

# Prints the model performance (Test set)
models_test

y_test

# Plot of Accuracy
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(5, 10))
sns.set_theme(style="whitegrid")
ax = sns.barplot(y=models_train.index, x="Accuracy", data=models_train)
ax.set(xlim=(0, 1))

# Plot of MCC
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(5, 10))
sns.set_theme(style="whitegrid")
ax = sns.barplot(y=models_train.index, x="matthews_corrcoef", data=models_train)
ax.set(xlim=(0, 1))

"""---

# **4 ML algorithms tried**

1. **SVM**
"""

from sklearn.svm import SVC
classifier = SVC(kernel='rbf', random_state = 1)
classifier.fit(X_train,y_train)

Y_pred = classifier.predict(X_test)

"""SVM EVALUATION"""

#accuracy of SVM
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test,Y_pred)
accuracy = float(cm.diagonal().sum())/len(y_test)

accuracy

from sklearn.metrics import f1_score
f1_score(y_test, Y_pred)

from sklearn.metrics import confusion_matrix

confusion_matrix = confusion_matrix(y_test,Y_pred, labels=[1,0])

confusion_matrix

"""**2. K NEAREST NEIGHBOUR ALGORITHM**"""

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=7)
knn.fit(X_train, y_train)
Y_pred_KNN = knn.predict(X_test)

"""KNN EVALUATION"""

#Accuracy
from sklearn.metrics import accuracy_score
accuracy_score(y_test, Y_pred_KNN)

f1_score(y_test, Y_pred_KNN)

from sklearn.metrics import confusion_matrix

confusion_matrix_KNN = confusion_matrix(y_test,Y_pred_KNN, labels=[1,0])

confusion_matrix_KNN

"""**3. DECISION TREE LEARNING**"""

import numpy as np
from matplotlib import rcParams

from sklearn.tree import DecisionTreeClassifier as dtc
from sklearn.metrics import accuracy_score
from sklearn.tree import plot_tree 
from termcolor import colored as cl

rcParams['figure.figsize'] = (25, 20)

model = dtc(criterion = 'entropy', max_depth = 4)
model.fit(X_train, y_train)

Y_pred_DT = model.predict(X_test)

accuracy_score(y_test, Y_pred_DT)

#f1dt
f1_score(y_test, Y_pred_DT)

from sklearn.metrics import confusion_matrix

confusion_matrix_DT = confusion_matrix(y_test,Y_pred_DT, labels=[1,0])

confusion_matrix_DT

"""VISUALIZING DECISION TREE"""

feature_names = feature.columns[:20]
target_names = feature['class'].unique().tolist()

plot_tree(model, 
          feature_names = feature_names, 
          class_names = target_names,
          fontsize = 9, 
          filled = True, 
          rounded = True)

plt.savefig('tree_visualization.png', dpi=100)

"""**4. Nu SVC ALGORITHM**"""

from sklearn.svm import NuSVC
from sklearn.model_selection import cross_val_score
from sklearn.metrics import confusion_matrix

nsvc = NuSVC()
nsvc.fit(X_train, y_train)

score = nsvc.score(X_train, y_train)

score

Y_pred_Nu = nsvc.predict(X_test)
accuracy_score(y_test, Y_pred_Nu)

"""**5. RANDOM FOREST**"""

# Build random forest model

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=500)

rf.fit(X_train, y_train)

"""### **Apply the model to make predictions**"""

y_train_pred = rf.predict(X_train)
y_test_pred = rf.predict(X_test)

"""### **Model performance**"""

feature['class']

# Simplest and quickest way to obtain the model performance (Accuracy)
accuracy_score(y_test, y_test_pred)

#rf.score(X_test,y_test)

from sklearn.metrics import confusion_matrix

confusion_matrix_RF = confusion_matrix(y_test,y_test_pred, labels=[1,0])

confusion_matrix_RF

# Matthew Correlation Coefficient
from sklearn.metrics import matthews_corrcoef

matthews_corrcoef(y_test, y_test_pred)

# Classification report
from sklearn.metrics import classification_report

model_report = classification_report(y_train, y_train_pred, target_names=['positive','negative'])

f = open('model_report.txt','w')
f.writelines(model_report) 
f.close()

# ROC curve
import matplotlib.pyplot as plt
from sklearn.metrics import plot_roc_curve

plot_roc_curve(rf, X_test, y_test)  
plt.show()

plot_roc_curve(rf, X_train, y_train)  
plt.show()

"""### **Feature importance**"""

# Display Dataframe of the dataset after feature selection (variance threshold)
X2

# Retrieve feature importance from the RF model
importance = pd.Series(rf.feature_importances_, name = 'Gini')

# Retrieve feature names
feature_names = pd.Series(X2.columns, name = 'Feature')

# Combine feature names and Gini values into a Dataframe
df = pd.concat([feature_names, importance], axis=1, names=['Feature', 'Gini'])
df

# Plot of feature importance
import matplotlib.pyplot as plt
import seaborn as sns

df_sorted = df.sort_values('Gini', ascending=False)[:20] # Sort by Gini in descending order; Showing only the top 20 results

plt.figure(figsize=(5, 10))
sns.set_theme(style="whitegrid")
ax = sns.barplot(x = 'Gini', y = 'Feature', data = df_sorted)
plt.xlabel("Feature Importance")

"""---"""